{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определение языка (language detection)\n",
    "--------------------\n",
    "\n",
    "* **Множество случаев** — тексты на разных языках\n",
    "* **Множество классов** — языки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я решила взять 4 языка, не очень связанных друг с другом. Это монгольский, польский, русинский и немецкий.\n",
    "Первым делом выкачиваем статьи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "langues = ['mn', 'pl', 'rue', 'de']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_texts_for_lang(lang, n=10): # функция для скачивания статей из википедии\n",
    "    wikipedia.set_lang(lang)\n",
    "    wiki_content = []\n",
    "    pages = wikipedia.random(n)\n",
    "    for page_name in pages:\n",
    "        try:\n",
    "            page = wikipedia.page(page_name)\n",
    "        except wikipedia.exceptions.WikipediaException:\n",
    "            print('Skipping page {}'.format(page_name))\n",
    "            continue\n",
    "\n",
    "        wiki_content.append('{}\\n{}'.format(page.title, page.content.replace('==', '')))\n",
    "\n",
    "    return wiki_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\1\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping page Араб\n",
      "Skipping page Анадырь (салаа утга)\n",
      "mn 98\n",
      "Skipping page Herat\n",
      "Skipping page Sing-Sing (album)\n",
      "Skipping page Pantone\n",
      "Skipping page Rezonans magnetyczny\n",
      "Skipping page Zarośla (Dąbrowa)\n",
      "Skipping page Thomas Hicks\n",
      "pl 94\n",
      "rue 100\n",
      "Skipping page Tom Kelly Band\n",
      "Skipping page Emden (Begriffsklärung)\n",
      "Skipping page Tarnow\n",
      "Skipping page Partridge\n",
      "Skipping page Wauchope\n",
      "Skipping page Ardenne\n",
      "Skipping page Kurt Runge (Ruderer)\n",
      "Skipping page Hans Häberlin\n",
      "de 92\n"
     ]
    }
   ],
   "source": [
    "import wikipedia # скачиваем по 100 статей для каждого языка. Это может занять какое-то время (5-10 минут. как правило)\n",
    "\n",
    "wiki_texts = {}\n",
    "for lang in langues: # казахский в википедии — это kk,\n",
    "                                      # украинский — uk, а белорусский — be\n",
    "    wiki_texts[lang] = get_texts_for_lang(lang, 100)\n",
    "    print(lang, len(wiki_texts[lang]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяем, скачались ли тексты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лихтенштайн\n",
      "Лихтенштайн (нім. Fürstentum Liechtenstein) — країна (конштітучна монархія) лежача в западній Европі. Лихтенштайн гранічіть з Австріёв, з Швейцаріёв. Головне місто є Вадуц. Популація — 36 476 (31.12.2011) жытелїв. Площа 160 км²;\n",
      "\n",
      "\n",
      " Названя \n",
      "\n",
      "\n",
      " Ґеоґрафія \n",
      "\n",
      "\n",
      " Адміністратівне дїлїня \n",
      "\n",
      "\n",
      " Населїня \n",
      "\n",
      "\n",
      " Історія \n",
      "Noah Saavedra\n",
      "Noah Saavedra (* 13. Februar 1991 in Oberpullendorf, Burgenland) ist ein österreichischer Filmschauspieler.\n",
      "\n",
      "\n",
      " Leben \n",
      "Noah Saavedra hat chilenische Wurzeln; sein Großvater war wegen des Militärregimes unter Augusto Pinochet von Chile nach Österreich geflohen. Noah Saavedra spricht daher auch fließend Spanisch.\n",
      "Nach dem Pflichtschulabschluss sammelte er von 2012 bis 2013 am Burgtheater in Wien erste Schauspielerfahrungen. Danach studierte er von 2013 bis 2015 Schauspiel am Konservatorium der Stadt Wien. Seit Herbst 2015 ist Saavedra Student an der Hochschule für Schauspielkunst „Ernst Busch“ Berlin eingeschrieben.\n",
      "Seine erste Filmrolle übernahm er 2015 im James Bond-Abenteuer Spectre, welches in Österreich gedreht wurde und in welchem er einen Snowboarder verkörperte. 2016 ist er als Egon Schiele in Egon Schiele: Tod und Mädchen zu sehen.\n",
      "Neben seiner Arbeit als Schauspieler ist Saavedra als Model tätig.\n",
      "\n",
      "\n",
      " Filmografie (Auswahl) \n",
      "2015: James Bond 007: Spectre  (Spectre)\n",
      "2016: Egon Schiele: Tod und Mädchen\n",
      "2018: Der Zürich-Krimi: Borchert und die Macht der Gewohnheit\n",
      "\n",
      "\n",
      " Auszeichnungen \n",
      "Romyverleihung 2017 – Bester Nachwuchs männlich\n",
      "\n",
      "\n",
      " Weblinks \n",
      "Noah Saavedra in der Internet Movie Database (englisch)\n",
      "\n",
      "\n",
      " Einzelnachweise \n"
     ]
    }
   ],
   "source": [
    "print(wiki_texts['rue'][0])\n",
    "print(wiki_texts['de'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Первый метод: частотные слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import codecs\n",
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from string import punctuation, digits\n",
    "punctuation = set(punctuation + '«»—…“”\\n\\t' + digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С очисткой у меня возникли небольшие проблемы. У знаков отсутствовала какая-либо закономерность, поэтому многие из них пришлось удалять отдельно, чтобы уж наверняка. \n",
    "Очистку вынесла в функцию, так как ниже использовала её отдельно от токенизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text = re.sub(r'\\.| - =|,|; !', ' ', text)\n",
    "    text = re.sub(r'=|·', ' ', text)\n",
    "    text = re.sub(r';', ' ', text)\n",
    "    text = re.sub(r'\\+|/', ' ', text)\n",
    "    text = re.sub(r'’|\\'', ' ', text)\n",
    "    text = re.sub('(.*?)-|:(.*?)', ' ', text)\n",
    "    text = re.sub(r'\\d|  |\\[|\\]|\\(|\\)|\\\\|—|\\\"|\\„|\\n|&|»|«', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = clean(text)\n",
    "    return text.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для собирания частотных слов. Возвращает сет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_freq(wiki_texts, lang):\n",
    "    freqs = collections.defaultdict(lambda: 0)\n",
    "    corpus = wiki_texts[lang]\n",
    "    for article in corpus:\n",
    "        for word in tokenize(article.replace('\\n', '').lower()):\n",
    "            freqs[word] += 1\n",
    "    return set(freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_de = get_freq(wiki_texts, 'de')\n",
    "freq_rue = get_freq(wiki_texts, 'rue')\n",
    "freq_pl = get_freq(wiki_texts, 'pl')\n",
    "freq_mn = get_freq(wiki_texts, 'mn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очищаем сеты от повторений и кладём их в словарь формата {'язык':'частотные слова'}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_freq = {}\n",
    "dict_freq['de'] = freq_de - freq_rue - freq_pl - freq_mn\n",
    "dict_freq['rue'] = freq_rue - freq_de - freq_pl - freq_mn\n",
    "dict_freq['pl'] = freq_pl - freq_de - freq_rue - freq_mn\n",
    "dict_freq['mn'] = freq_mn - freq_de - freq_rue - freq_pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция по определению языка на основе частотных слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_language_f(text, dict_freq, langues):\n",
    "    text_chars = tokenize(text.lower())\n",
    "    lang2sim = {}\n",
    "    \n",
    "    for lang in langues:\n",
    "        intersect = len(set(text_chars) & dict_freq[lang])\n",
    "        lang2sim[lang] = intersect\n",
    "    \n",
    "    return max(lang2sim, key=lambda x: lang2sim[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Второй метод: частотные символьные n-граммы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция, которая преобразовывает строку в массив n-грамм заданной длины."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import islice, tee\n",
    "\n",
    "def make_ngrams(text):\n",
    "    N = 3 # задаем длину n-граммы\n",
    "    text = clean(text)\n",
    "    text = text.strip()\n",
    "    ngrams = zip(*(islice(seq, index, None) for index, seq in enumerate(tee(text, N))))\n",
    "    ngrams_fin = [''.join(x) for x in ngrams]\n",
    "    return ngrams_fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для собирания частотных n-грамм. Возвращает сет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(wiki_texts, lang):\n",
    "    ngrams_l= collections.defaultdict(lambda: 0)\n",
    "    corpus = wiki_texts[lang]\n",
    "    for article in corpus:\n",
    "        for ngram in make_ngrams(article.replace('\\n', '').lower()):\n",
    "            ngrams_l[ngram] += 1\n",
    "    return set(ngrams_l.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngrams_fr = get_ngrams(wiki_texts, 'de')\n",
    "ngrams_uk = get_ngrams(wiki_texts, 'rue')\n",
    "ngrams_be = get_ngrams(wiki_texts, 'pl')\n",
    "ngrams_kk = get_ngrams(wiki_texts, 'mn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очищаем сеты от повторений и кладём в словарь формата {'язык':'n-граммы'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_ngrams = {}\n",
    "dict_ngrams['de'] = ngrams_de - ngrams_rue - ngrams_pl - ngrams_mn\n",
    "dict_ngrams['rue'] = ngrams_rue - ngrams_de - ngrams_pl - ngrams_mn\n",
    "dict_ngrams['pl'] = ngrams_pl - ngrams_de - ngrams_rue - ngrams_mn\n",
    "dict_ngrams['mn'] = ngrams_mn - ngrams_de - ngrams_rue - ngrams_pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция по определению языка на основе частотных слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_language_n(text, dict_ngrams, langues):\n",
    "    text_chars = make_ngrams(text.lower())\n",
    "    lang2sim = {}\n",
    "    \n",
    "    for lang in langues:\n",
    "        intersect = len(set(text_chars) & dict_ngrams[lang])\n",
    "        lang2sim[lang] = intersect\n",
    "    \n",
    "    return max(lang2sim, key=lambda x: lang2sim[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определяем язык"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_lang(wiki_texts, langues, dict_freq, dict_ngrams):\n",
    "    check_n = []\n",
    "    check_f = []\n",
    "    for lang in langues:\n",
    "        for texts in wiki_texts[lang]:\n",
    "            c_f = predict_language_f(texts, dict_freq, langues)\n",
    "            check_f.append(c_f == lang)\n",
    "            c_n = predict_language_n(texts, dict_ngrams, langues)\n",
    "            check_n.append(c_n == lang)\n",
    "            \n",
    "    return check_f, check_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_freq, pred_ngram = predict_lang(wiki_texts, langues, dict_freq, dict_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "False in pred_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "False in ngram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оба метода определяют язык без ошибки."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
