{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определение языка (language detection)\n",
    "--------------------\n",
    "\n",
    "* **Множество случаев** — тексты на разных языках\n",
    "* **Множество классов** — языки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Абсолютно все свои заметки я написала здесь. Надеюсь, это будет считаться в качестве отчёта."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я решила взять 4 языка, не очень связанных друг с другом. Это монгольский, польский, русинский и немецкий. Никакой особой логики в выборе языков нет, просто было интересно.\n",
    "Первым делом выкачиваем статьи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "langues = ['mn', 'pl', 'rue', 'de']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_texts_for_lang(lang, n=10): # функция для скачивания статей из википедии\n",
    "    wikipedia.set_lang(lang)\n",
    "    wiki_content = []\n",
    "    pages = wikipedia.random(n)\n",
    "    for page_name in pages:\n",
    "        try:\n",
    "            page = wikipedia.page(page_name)\n",
    "        except wikipedia.exceptions.WikipediaException:\n",
    "            print('Skipping page {}'.format(page_name))\n",
    "            continue\n",
    "\n",
    "        wiki_content.append('{}\\n{}'.format(page.title, page.content.replace('==', '')))\n",
    "\n",
    "    return wiki_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изначально я делала на том же составе языков, что был дан в примере. И по какой-то причине он работает гораздо быстрее, чем мой. В первый раз на выкачку статей ушло пол часа, потом стандартно минут 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping page 2011-2012 оны НБА-гийн улирал\n",
      "mn 99\n",
      "Skipping page Beveren (stacja kolejowa)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\1\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping page Złoty Krążek (Słowacja)\n",
      "Skipping page Coppa Italia\n",
      "Skipping page Kaplica papieska\n",
      "Skipping page Blackout\n",
      "pl 95\n",
      "rue 100\n",
      "Skipping page Gipshütte\n",
      "Skipping page Karl Findeisen\n",
      "Skipping page Bundesgericht\n",
      "Skipping page Großer Vogelsand\n",
      "Skipping page Tanck\n",
      "Skipping page Erich Zieger\n",
      "Skipping page Gaius Claudius Pulcher\n",
      "Skipping page Richard Portman\n",
      "Skipping page Krieger\n",
      "de 91\n"
     ]
    }
   ],
   "source": [
    "import wikipedia # скачиваем по 100 статей для каждого языка. Это может занять какое-то время (5-10 минут. как правило)\n",
    "\n",
    "wiki_texts = {}\n",
    "for lang in langues:\n",
    "    wiki_texts[lang] = get_texts_for_lang(lang, 100)\n",
    "    print(lang, len(wiki_texts[lang]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяем, скачались ли тексты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "922\n",
      "\n",
      " Подїї \n",
      "\n",
      "\n",
      " Народили ся \n",
      "\n",
      "\n",
      " Вмерли \n",
      "Spitalerstraße\n",
      "Die Spitalerstraße in Hamburg ist eine der zentralen Einkaufsstraßen der Stadt. Sie verbindet den mittleren Teil der Mönckebergstraße und den Gerhart-Hauptmann-Platz auf Höhe des U-Bahnhof Mönckebergstraße mit den östlich gelegenen, ineinander übergehenden Straßen Glockengießerwall und Steintorwall gegenüber dem westlichen Eingang zur Wandelhalle des Hauptbahnhofs.\n",
      "Nach Passantenzählungen in Einkaufsstraßen war die Spitalerstraße 2013 mit 9.215 Fußgängern in einer Stunde, nach der Mönckebergstraße, die zweitstärkst frequentierte Straße in Hamburg. Im bundesweiten Vergleich rangierte die Spitalerstraße im Jahr 2013 auf Platz 12 und lag im Jahr 2011 sogar mit 11.190 Fußgängern pro Stunde auf Platz 7 nur knapp hinter der Mönckebergstraße.\n",
      "Außer den Blickachsen auf die Halle des Hauptbahnhofs im Osten und die am Rande der Achse stehende Petrikirche im Westen wird die Spitalerstraße wesentlich durch Geschäftsgebäude geprägt, die überwiegend seit etwa 1900 gebaut wurden, teilweise auch aus der Nachkriegszeit stammen. Mit den Gebäuden Barkhof, Seeburg und Semperhaus stehen drei der großen Kontorhäuser in der Spitalerstraße. Die Straße ist auf ganzer Länge als Fußgängerzone ausgebaut, wobei inzwischen typische Konzern-Filialisten das Bild beherrschen, während sich in den 1970er und 1980er Jahren auch Einzelhandelsgeschäfte dort fanden.\n",
      "\n",
      "\n",
      " Geschichte \n",
      "\n",
      "Die Benennung der Spitalerstraße fußt darauf, dass zu früheren Zeiten in der Stadt zweimal wöchentlich Almosen in Form von Lebensmitteln zu Gunsten „der armen Seeken [Siechen]“ gesammelt und „up dem Stiege to St. Jürgen“ – dem 1190 erstmals erwähnten Hospital St. Georg – gebracht wurden. Dieser, von der Stadt durch den Wald nach dem Siechen-Spital St. Georg führende Stieg erhielt davon den Namen „Spitaler Straße“ und das später an deren Ausgang an der Stadtmauer erbaute Tor: „Spitaler Tor“.Auf alten Stadtplänen um 1200 ist der Verlauf bis hinaus zum „Secken“ – bzw. „St. Georgs Hospital“ verfolgbar.Mit der wachsenden Ausdehnung Hamburgs wurde um 1300 der zum Hospital führende „Stieg“ zu einer innerstädtischen Straße mit einem eigenen „Spitaler Tor“ in der Stadtmauer und auf einem Stadtplan von 1320 ist die Straße auch namentlich als Spitalerstrate benannt. Das am damaligen Ende der Spitalerstraße gelegene Hiobs-Hospital wird gelegentlich als Namensgeber vermutet, doch ist es erst 1509 errichtet worden, als die Benennung der Straße längst existierte.\n",
      "Mit dem Bau der Hamburger Wallanlagen um 1620 wurde das Spitaler Tor geschlossen und in eine Bastion umgewandelt, die Spitalerstraße hatte damit keine direkte Fortführung mehr zum St.-Georgs-Hospital, sie endete bis etwa 1830 am Straßenzug Kurze Mühren/Lange Mühren und dem damaligen Schweinemarkt.\n",
      "Die Häuser der dunklen Gasse überstanden den großen Brand von 1842 weitgehend unbeschadet. Erst die Folgen der Choleraepidemie von 1892 führten zu umfangreichen Sanierungsmaßnahmen, die zwischen 1906 und 1909 die Straße zu einer modernen und vornehmen Geschäftsstraße mit Kontorhäusern wie dem Barkhof, dem Südseehaus und den Semperhäusern umwandelten. Büros von Handelshäusern belegten die Obergeschosse, während die Erdgeschosse häufig als Verkaufsräume vermietet wurden. Die zentrale Lage und die Nähe zum Hauptbahnhof ließen die Spitalerstraße zu einer der attraktivsten Einkaufsstraßen Hamburgs werden, wobei allerdings zunehmend der wachsende Autoverkehr störte. Letzteres führte dazu, dass die Straße zum 7. September 1968 in eine Fußgängerzone umgewandelt wurde.Zwischenzeitlich war um 1880 die Spitalerstraße im Osten bis zu den Straßen Glockengießerwall und Steintorwall fortgeführt worden.\n",
      "Die bis etwa 1915 mit „Breite Straße“ benannte Fläche zwischen dem damaligen Pferdemarkt und dem westlichen Ende der Spitalerstraße ist heute gemäß der postalischen Adresse der Spitalerstraße zugeordnet.\n",
      "\n",
      "\n",
      " Fotografien und Karte \n",
      "\n",
      "\t\t\n",
      "\t\t\n",
      "\t\t\n",
      "\t\t\n",
      "\n",
      "\n",
      " Einzelnachweise \n",
      "\n",
      "\n",
      " Weblinks \n",
      "\n",
      "„Karte von Hamburg 1868“ mit Straßensuche und interaktiver Karte\n"
     ]
    }
   ],
   "source": [
    "print(wiki_texts['rue'][0])\n",
    "print(wiki_texts['de'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Первый метод: частотные слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import codecs\n",
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from string import punctuation, digits\n",
    "punctuation = set(punctuation + '«»—…“”\\n\\t' + digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С очисткой у меня возникли небольшие проблемы. У знаков отсутствовала какая-либо закономерность, поэтому многие из них пришлось удалять отдельно, чтобы уж наверняка. \n",
    "Очистку вынесла в функцию, так как ниже использовала её отдельно от токенизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text = re.sub(r'\\.| - =|,|; !', ' ', text)\n",
    "    text = re.sub(r'=|·', ' ', text)\n",
    "    text = re.sub(r';', ' ', text)\n",
    "    text = re.sub(r'\\+|/', ' ', text)\n",
    "    text = re.sub(r'’|\\'', ' ', text)\n",
    "    text = re.sub('(.*?)-|:(.*?)', ' ', text)\n",
    "    text = re.sub(r'\\d|  |\\[|\\]|\\(|\\)|\\\\|—|\\\"|\\„|\\n|&|»|«', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = clean(text)\n",
    "    return text.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для собирания частотных слов. Возвращает сет частотных слов для языка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_freq(wiki_texts, lang):\n",
    "    freqs = collections.defaultdict(lambda: 0)\n",
    "    corpus = wiki_texts[lang]\n",
    "    for article in corpus:\n",
    "        for word in tokenize(article.replace('\\n', '').lower()):\n",
    "            freqs[word] += 1\n",
    "    return set(freqs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freq_de = get_freq(wiki_texts, 'de')\n",
    "freq_rue = get_freq(wiki_texts, 'rue')\n",
    "freq_pl = get_freq(wiki_texts, 'pl')\n",
    "freq_mn = get_freq(wiki_texts, 'mn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очищаем сеты от повторений и кладём их в словарь формата {'язык':'частотные слова'}. В немецком (и не только в нём) мне попадались иероглифы. Удалить их никак не смогла, так как не знаю точно, сколько их и какие. Но этот факт не повлиял на результаты, поэтому оставила так, как есть."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_freq = {}\n",
    "dict_freq['de'] = freq_de - freq_rue - freq_pl - freq_mn\n",
    "dict_freq['rue'] = freq_rue - freq_de - freq_pl - freq_mn\n",
    "dict_freq['pl'] = freq_pl - freq_de - freq_rue - freq_mn\n",
    "dict_freq['mn'] = freq_mn - freq_de - freq_rue - freq_pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция по определению языка на основе частотных слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_language_f(text, dict_freq, langues):\n",
    "    text_chars = tokenize(text.lower())\n",
    "    lang2sim = {}\n",
    "    \n",
    "    for lang in langues:\n",
    "        intersect = len(set(text_chars) & dict_freq[lang])\n",
    "        lang2sim[lang] = intersect\n",
    "    \n",
    "    return max(lang2sim, key=lambda x: lang2sim[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Второй метод: частотные символьные n-граммы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция, которая преобразовывает строку в массив n-грамм заданной длины."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import islice, tee\n",
    "\n",
    "def make_ngrams(text):\n",
    "    N = 3 # задаем длину n-граммы\n",
    "    text = clean(text)\n",
    "    text = text.strip()\n",
    "    ngrams = zip(*(islice(seq, index, None) for index, seq in enumerate(tee(text, N))))\n",
    "    ngrams_fin = [''.join(x) for x in ngrams]\n",
    "    return ngrams_fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для собирания частотных n-грамм. Возвращает сет n-грамм для языка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ngrams(wiki_texts, lang):\n",
    "    ngrams_l= collections.defaultdict(lambda: 0)\n",
    "    corpus = wiki_texts[lang]\n",
    "    for article in corpus:\n",
    "        for ngram in make_ngrams(article.replace('\\n', '').lower()):\n",
    "            ngrams_l[ngram] += 1\n",
    "    return set(ngrams_l.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngrams_de = get_ngrams(wiki_texts, 'de')\n",
    "ngrams_rue = get_ngrams(wiki_texts, 'rue')\n",
    "ngrams_pl = get_ngrams(wiki_texts, 'pl')\n",
    "ngrams_mn = get_ngrams(wiki_texts, 'mn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очищаем сеты от повторений и кладём в словарь формата {'язык':'n-граммы'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ngrams = {}\n",
    "dict_ngrams['de'] = ngrams_de - ngrams_rue - ngrams_pl - ngrams_mn\n",
    "dict_ngrams['rue'] = ngrams_rue - ngrams_de - ngrams_pl - ngrams_mn\n",
    "dict_ngrams['pl'] = ngrams_pl - ngrams_de - ngrams_rue - ngrams_mn\n",
    "dict_ngrams['mn'] = ngrams_mn - ngrams_de - ngrams_rue - ngrams_pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция по определению языка на основе частотных слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_language_n(text, dict_ngrams, langues):\n",
    "    text_chars = make_ngrams(text.lower())\n",
    "    lang2sim = {}\n",
    "    \n",
    "    for lang in langues:\n",
    "        intersect = len(set(text_chars) & dict_ngrams[lang])\n",
    "        lang2sim[lang] = intersect\n",
    "    \n",
    "    return max(lang2sim, key=lambda x: lang2sim[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определяем язык"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_lang(wiki_texts, langues, dict_freq, dict_ngrams):\n",
    "    check_n = []\n",
    "    check_f = []\n",
    "    for lang in langues:\n",
    "        for texts in wiki_texts[lang]:\n",
    "            c_f = predict_language_f(texts, dict_freq, langues)\n",
    "            check_f.append(c_f == lang)\n",
    "            #check_f.append(lang)\n",
    "            c_n = predict_language_n(texts, dict_ngrams, langues)\n",
    "            check_n.append(c_n == lang)\n",
    "            \n",
    "    return check_f, check_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_freq, pred_ngram = predict_lang(wiki_texts, langues, dict_freq, dict_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "False in pred_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de\n"
     ]
    }
   ],
   "source": [
    "for i in range (0, 770):\n",
    "    if pred_freq[i] is False:\n",
    "        print(pred_freq[i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "False in pred_ngram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод частотных слов выдаёт одну ошибку в предложении на немецком языке. Немецкий язык определяется как русинский.\n",
    "Теперь сделаем проверку на другой выборке текстов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mn 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\1\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping page (140) Siwa\n",
      "Skipping page Saint-Estèphe\n",
      "Skipping page Zarzecze (obwód lwowski)\n",
      "Skipping page Pleuroceras\n",
      "Skipping page Marigot\n",
      "pl 95\n",
      "rue 100\n",
      "Skipping page Mitsuru Satō\n",
      "Skipping page ALJ\n",
      "Skipping page Adolf Sonnenschein (Verwaltungsjurist)\n",
      "Skipping page Wilhelm Schacht\n",
      "Skipping page Häselbach\n",
      "Skipping page Schiffskiel\n",
      "Skipping page Ninowa\n",
      "Skipping page Blesendorf\n",
      "Skipping page Tayabas\n",
      "Skipping page Yellowstone Township\n",
      "de 90\n"
     ]
    }
   ],
   "source": [
    "wiki_texts_check = {}\n",
    "for lang in langues:\n",
    "    wiki_texts_check[lang] = get_texts_for_lang(lang, 100)\n",
    "    print(lang, len(wiki_texts_check[lang]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_lang(wiki_texts_check, langues, dict_freq, dict_ngrams):\n",
    "    check_n = []\n",
    "    check_f = []\n",
    "    for lang in langues:\n",
    "        for texts in wiki_texts_check[lang]:\n",
    "            c_f = predict_language_f(texts, dict_freq, langues)\n",
    "            check_f.append(c_f == lang)\n",
    "            #check_f.append(lang)\n",
    "            c_n = predict_language_n(texts, dict_ngrams, langues)\n",
    "            check_n.append(c_n == lang)\n",
    "            #check_n.append(lang)\n",
    "            \n",
    "    return check_f, check_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_freq_check, pred_ngram_check = predict_lang(wiki_texts_check, langues, dict_freq, dict_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "False in pred_freq_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rue\n"
     ]
    }
   ],
   "source": [
    "for i in range (0, 770):\n",
    "    if pred_freq_check[i] is False:\n",
    "        print(pred_freq_check[i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "770"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred_freq_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "False in pred_ngram_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mn\n",
      "de\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 770):\n",
    "    if pred_ngram_check[i] is False:\n",
    "        print(pred_ngram_check[i+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат немного ухудшился. Однако нельзя сказать, что это зависит от какого-то конкретного языка, так как на тестовой выборке ошибки появляются в разных языках. Единственный язык, который всегда определяется правильно - польский.\n",
    "В случае n-грамм монгольский и немецкий языки по одному разу определяются как польский. В случае частотных слов русинский определяется как монгольский. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я считаю, что оба метода работают одинаково хорошо. Ошибки в обоих случаях единичны и не систематичны. Эти языки не похожи, возможн, ошибка возникла из-за иноязычных вкраплений в статьях википедии.\n",
    "В эту тетрадку не вошло, но до финальной записи я проверяла превоначальный набор языков на этих же функциях (фр, ук, бел, каз), и ошибок не было."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
